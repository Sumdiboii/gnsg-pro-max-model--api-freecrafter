# -*- coding: utf-8 -*-
"""ggns-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHfPBecxXd8xtgimLY8OuSWIH6fU_VC1
"""

# Step 1: Install bing-image-downloader
!pip install bing-image-downloader pillow

# Step 2: Import
from bing_image_downloader import downloader
import os
from PIL import Image

# Step 3: Download 100 images of dogs
downloader.download("dog", limit=100, output_dir='dog_dataset', adult_filter_off=True, force_replace=False, timeout=60)

# Step 4: Resize images to 512x512
output_folder = "dog_dataset/dog"
for filename in os.listdir(output_folder):
    file_path = os.path.join(output_folder, filename)
    try:
        img = Image.open(file_path).convert("RGB")
        img = img.resize((512, 512))
        img.save(file_path)
    except:
        os.remove(file_path)  # remove corrupted files

print("✅ Done! All resized dog images are saved in 'dog_dataset/dog' folder.")

!pip install accelerate transformers diffusers==0.30.0 safetensors datasets torch torchvision peft pillow

import os, json

image_folder = "dog_dataset/dog"
metadata_path = os.path.join(image_folder, "metadata.jsonl")

with open(metadata_path, "w") as f:
    for file in os.listdir(image_folder):
        if file.lower().endswith((".png", ".jpg", ".jpeg")):
            entry = {"file_name": file, "text": "a photo of a dog"}
            f.write(json.dumps(entry) + "\n")

print("✅ metadata.jsonl created at:", metadata_path)

from datasets import load_dataset

dataset = load_dataset(
    "imagefolder",
    data_dir="dog_dataset",
    split="train"
)

print(dataset)

from torchvision import transforms
from transformers import CLIPTokenizer
import torch

tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

image_transforms = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor()
])

def preprocess(example):
    img = example["image"]
    if not isinstance(img, torch.Tensor):
        img = image_transforms(img)
    # Stable Diffusion needs 4 channels
    if img.shape[0] == 3:
        alpha = torch.ones(1, img.shape[1], img.shape[2])
        img = torch.cat([img, alpha], dim=0)
    example["pixel_values"] = img

    tokens = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=tokenizer.model_max_length,
        return_tensors="pt"
    )
    example["input_ids"] = tokens.input_ids.squeeze(0)
    return example

dataset = dataset.map(preprocess)
print("✅ Preprocessing complete!")

dataset.save_to_disk("dog_dataset_hf")
print("✅ Dataset saved at dog_dataset_hf")

from datasets import load_from_disk

dataset = load_from_disk("dog_dataset_hf")
print(f"✅ Dataset loaded with {len(dataset)} images")

from diffusers import DDPMScheduler, UNet2DConditionModel
from transformers import CLIPTextModel
from peft import LoraConfig, get_peft_model

model_id = "stabilityai/stable-diffusion-2-1-base"

text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["to_q", "to_v"],
    lora_dropout=0.05,
    bias="none"
)
unet = get_peft_model(unet, lora_config)

from torch.utils.data import DataLoader
import torch

def collate_fn(batch):
    # Ensure pixel_values are tensors
    pixel_values = torch.stack([
        torch.tensor(item["pixel_values"]) if isinstance(item["pixel_values"], list) else item["pixel_values"]
        for item in batch
    ])

    # Ensure input_ids are tensors
    input_ids = torch.stack([
        torch.tensor(item["input_ids"]) if isinstance(item["input_ids"], list) else item["input_ids"]
        for item in batch
    ])

    return {"pixel_values": pixel_values, "input_ids": input_ids}

train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

unet.to(device)
text_encoder.to(device)

optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)

# Set dataset format
dataset.set_format(type="torch", columns=["pixel_values", "input_ids"])
train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

for epoch in range(num_epochs):
    print(f"=== Epoch {epoch+1}/{num_epochs} ===")
    for batch in train_dataloader:
        pixel_values = batch["pixel_values"].to(device, dtype=torch.float16)
        input_ids = batch["input_ids"].to(device)

        noise = torch.randn_like(pixel_values)
        timesteps = torch.randint(
            0, noise_scheduler.num_train_timesteps,
            (pixel_values.shape[0],), device=device
        ).long()
        noisy_images = noise_scheduler.add_noise(pixel_values, noise, timesteps)

        with torch.autocast("cuda", dtype=torch.float16):
            # Freeze text encoder to save memory
            with torch.no_grad():
                encoder_hidden_states = text_encoder(input_ids).last_hidden_state
            noise_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample
            loss = torch.nn.functional.mse_loss(noise_pred, noise)

        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print(f"Epoch {epoch+1} finished, loss: {loss.item():.4f}")

# Save LoRA weights only
unet.save_pretrained("dog_unet_lora")
print("✅ LoRA weights saved at dog_unet_lora")

text_encoder.save_pretrained("dog_text_encoder")
print("✅ Text encoder saved")

from diffusers import StableDiffusionPipeline, DDPMScheduler, AutoencoderKL
from transformers import CLIPTokenizer, CLIPTextModel
from peft import PeftModel
import torch

device = "cuda"

# Base model
model_id = "stabilityai/stable-diffusion-2-1-base"

# Load tokenizer and text encoder
tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")

# Load UNet
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
# Apply your trained LoRA weights
unet = PeftModel.from_pretrained(unet, "dog_unet_lora")

# Load VAE
vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae")

# Scheduler
scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")

# Build pipeline
pipe = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    unet=unet,
    tokenizer=tokenizer,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None
).to(device)

# Generate an image
prompt = "A cute dog sitting in a garden, digital art"
image = pipe(prompt, height=128, width=128, num_inference_steps=25).images[0]

# Save it
image.save("dog_generated.png")
print("✅ Image saved as dog_generated.png")

from PIL import Image
import matplotlib.pyplot as plt

# Prompt
prompt = "A cute dog sitting in a garden, digital art"

# Generate image
generated = pipe(prompt, height=128, width=128, num_inference_steps=25).images[0]

# Show in notebook
plt.imshow(generated)
plt.axis('off')
plt.show()

!pip install fastapi uvicorn pyngrok

!ngrok config add-authtoken 32UJcbLWmAx87AUqlh1kPdeUaHL_545osib8ktxoLFrnYFDDs

from fastapi import FastAPI
from pydantic import BaseModel
import torch, base64
from io import BytesIO
from diffusers import StableDiffusionPipeline, DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTokenizer, CLIPTextModel
from peft import PeftModel

device = "cuda" if torch.cuda.is_available() else "cpu"

# ----- Load model & your LoRA -----
model_id = "stabilityai/stable-diffusion-2-1-base"
tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder")
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet")
unet = PeftModel.from_pretrained(unet, "dog_unet_lora")      # <-- your LoRA weights
vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae")
scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")

pipe = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    unet=unet,
    tokenizer=tokenizer,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None
).to(device)

# ----- FastAPI app -----
app = FastAPI()

class Prompt(BaseModel):
    prompt: str
    height: int = 256
    width: int = 256
    steps: int = 30

@app.post("/generate")
def generate_image(data: Prompt):
    image = pipe(
        data.prompt,
        height=data.height,
        width=data.width,
        num_inference_steps=data.steps
    ).images[0]
    buf = BytesIO()
    image.save(buf, format="PNG")
    b64 = base64.b64encode(buf.getvalue()).decode()
    return {"image": b64}

from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Or specify your frontend URL for better security
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

from pyngrok import ngrok
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

!pip install nest_asyncio
import nest_asyncio
nest_asyncio.apply()

import uvicorn
uvicorn.run(app, host="0.0.0.0", port=8000)